---
phase: 01-data-layer-refactor
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - utils/data_loader.py
  - tests/test_data_loader.py
autonomous: true

must_haves:
  truths:
    - "App starts without data loading errors"
    - "All 7 tasi_optimized views load successfully"
    - "Cached loading returns same data on subsequent calls"
    - "Tests pass with new view-based structure"
  artifacts:
    - path: "utils/data_loader.py"
      provides: "Multi-view data loading from tasi_optimized"
      exports: ["load_tasi_data", "get_view", "get_view_info", "VIEW_NAMES"]
    - path: "tests/test_data_loader.py"
      provides: "Tests for new data loading structure"
      contains: "test_load_tasi_data"
  key_links:
    - from: "utils/data_loader.py"
      to: "data/tasi_optimized/*.parquet"
      via: "pd.read_parquet calls"
      pattern: "read_parquet.*tasi_optimized"
---

<objective>
Refactor data loading to use the new tasi_optimized parquet structure with 7 pre-optimized views.

Purpose: Replace legacy 4-file parquet loading with the new optimized 7-view structure, enabling query routing to optimal views.
Output: Updated data_loader.py that loads all tasi_optimized views with Streamlit caching, and updated tests that verify the new structure.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-data-layer-refactor/01-RESEARCH.md

# Current implementation
@utils/data_loader.py
@tests/test_data_loader.py

# New data structure
@data/tasi_optimized/metadata.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor data_loader.py for tasi_optimized views</name>
  <files>utils/data_loader.py</files>
  <action>
Refactor data_loader.py to load from tasi_optimized structure:

1. Update `load_data()` to `load_tasi_data()` that loads all 7 views:
   - tasi_financials (from tasi_financials.parquet)
   - latest_financials (from latest_financials.parquet)
   - latest_annual (from latest_annual.parquet)
   - ticker_index (from ticker_index.parquet)
   - company_annual_timeseries (from views/company_annual_timeseries.parquet)
   - sector_benchmarks_latest (from views/sector_benchmarks_latest.parquet)
   - top_bottom_performers (from views/top_bottom_performers.parquet)

2. Keep `@st.cache_data(show_spinner=False)` decorator for caching

3. Add TTL of 3600 seconds (1 hour) per research recommendation:
   `@st.cache_data(show_spinner=False, ttl=3600)`

4. Update path to use `tasi_optimized` subdirectory:
   `base_path = get_data_path() / "tasi_optimized"`

5. Add `get_view(name: str) -> pd.DataFrame` function to get a specific view

6. Add `VIEW_NAMES` constant with all valid view names

7. Update `get_view_info()` to return stats for new structure:
   - total_companies (from ticker_index)
   - total_records (from tasi_financials)
   - views_available (count of 7)

8. Remove legacy `get_dataset()`, `get_dataset_info()`, `get_column_info()` functions
   OR keep them as deprecated wrappers that map to new functions

9. Remove `DATASET_DISPLAY_NAMES` constant (no longer needed with auto-routing)

Use Path() for cross-platform compatibility. Log success/failure with logger.
  </action>
  <verify>
Run: `python -c "from utils.data_loader import load_tasi_data; d = load_tasi_data(); print(f'Loaded {len(d)} views:', list(d.keys()))"`
Expected output: "Loaded 7 views: ['tasi_financials', 'latest_financials', 'latest_annual', 'ticker_index', 'company_annual_timeseries', 'sector_benchmarks_latest', 'top_bottom_performers']"
  </verify>
  <done>
- load_tasi_data() returns dict with all 7 views as DataFrames
- get_view("tasi_financials") returns the full dataset DataFrame
- Cache decorator has TTL=3600
- Path uses tasi_optimized subdirectory
  </done>
</task>

<task type="auto">
  <name>Task 2: Update tests for new data structure</name>
  <files>tests/test_data_loader.py</files>
  <action>
Update test_data_loader.py to test the new view-based structure:

1. Replace all tests referencing old structure (filings, facts, ratios, analytics) with new view tests

2. Add test `test_load_tasi_data_returns_dict()`:
   - Verify returns dict
   - Verify all 7 view names present as keys

3. Add test `test_load_tasi_data_returns_dataframes()`:
   - Verify each value is a pd.DataFrame

4. Add test `test_get_view_returns_correct_data()`:
   - Test get_view("tasi_financials") returns same as load_tasi_data()["tasi_financials"]

5. Add test `test_get_view_invalid_name()`:
   - Verify ValueError raised for invalid view name

6. Add test `test_view_names_constant()`:
   - Verify VIEW_NAMES contains all 7 view names

7. Add test `test_tasi_financials_has_expected_columns()`:
   - Verify key columns exist: ticker, company_name, fiscal_year, revenue, net_profit

8. Add test `test_latest_financials_row_count()`:
   - Verify latest_financials has 302 rows (one per company)

9. Add test `test_ticker_index_has_required_columns()`:
   - Verify columns: ticker, company_name, sector

Remove old tests that reference legacy structure (filings, facts, ratios, analytics datasets).
  </action>
  <verify>
Run: `pytest tests/test_data_loader.py -v`
Expected: All tests pass
  </verify>
  <done>
- All new tests pass
- No tests reference legacy 4-file structure
- Tests verify 7 views load correctly
- Tests verify key columns exist in views
  </done>
</task>

</tasks>

<verification>
1. Data loads without errors:
   `python -c "from utils.data_loader import load_tasi_data; load_tasi_data()"`

2. All tests pass:
   `pytest tests/test_data_loader.py -v`

3. View count is correct:
   `python -c "from utils.data_loader import load_tasi_data; print(len(load_tasi_data()))"`
   Should print: 7
</verification>

<success_criteria>
1. load_tasi_data() returns dict with 7 DataFrames
2. All 7 parquet views accessible via get_view()
3. pytest tests/test_data_loader.py passes with 0 failures
4. No import errors when loading utils.data_loader
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-layer-refactor/01-01-SUMMARY.md`
</output>
